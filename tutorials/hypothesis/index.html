<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>p-value, null hypothesis and real time analytic · GeneralLedger.jl</title><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.png" alt="GeneralLedger.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">GeneralLedger.jl</a></span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Objective</a></li><li><a class="tocitem" href="../../introduction/">Introduction</a></li><li><span class="tocitem">Tutorials</span><ul><li><a class="tocitem" href="../aboutgl/">About GL</a></li><li><a class="tocitem" href="../glprocesses/">GL Processes</a></li><li><a class="tocitem" href="../erd/">ERD</a></li><li><a class="tocitem" href="../installation/">Installing Julia</a></li><li><a class="tocitem" href="../selfservice/">Self-Service Data Analytics</a></li><li><a class="tocitem" href="../plots/">Visualizations</a></li><li><input class="collapse-toggle" id="menuitem-3-7" type="checkbox" checked/><label class="tocitem" for="menuitem-3-7"><span class="docs-label">ML4GL</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../mlforgl/">Machine Learning</a></li><li><a class="tocitem" href="../financedata/">Finance Data</a></li><li><a class="tocitem" href="../probability/">Probability</a></li><li><a class="tocitem" href="../statistics/">Statistics</a></li><li class="is-active"><a class="tocitem" href>p-value, null hypothesis and real time analytic</a><ul class="internal"><li><a class="tocitem" href="#Hypothesis,-p-value"><span>Hypothesis, p - value</span></a></li><li><a class="tocitem" href="#what-is-a-gradient,-derivatives,-gradients,-Jacobians,-Hessians"><span>what is a gradient, derivatives, gradients, Jacobians, Hessians</span></a></li><li><a class="tocitem" href="#what-is-optimization"><span>what is optimization</span></a></li><li><a class="tocitem" href="#ForwardDiff,-ReverseDiff"><span>ForwardDiff, ReverseDiff</span></a></li><li><a class="tocitem" href="#ChainRules,-AutoDiff"><span>ChainRules, AutoDiff</span></a></li><li><a class="tocitem" href="#Optimization-using-gradient"><span>Optimization using gradient</span></a></li><li><a class="tocitem" href="#what-is-gradient-descent"><span>what is gradient descent</span></a></li><li><a class="tocitem" href="#UAT-Universal-Approximation-theorem"><span>UAT Universal Approximation theorem</span></a></li><li><a class="tocitem" href="#Linear-regression"><span>Linear regression</span></a></li><li><a class="tocitem" href="#Taylor-Series"><span>Taylor Series</span></a></li><li><a class="tocitem" href="#Fourier-Transformation"><span>Fourier Transformation</span></a></li><li><a class="tocitem" href="#Loss-function"><span>Loss function</span></a></li><li><a class="tocitem" href="#Gradient-and-Gradient-Descent"><span>Gradient &amp; Gradient Descent</span></a></li><li><a class="tocitem" href="#Curse-of-Dimensionality"><span>Curse of Dimensionality</span></a></li><li><a class="tocitem" href="#what-is-a-Neural-network"><span>what is a Neural network</span></a></li><li><a class="tocitem" href="#Neurons"><span>Neurons</span></a></li><li><a class="tocitem" href="#Why-we-need-layers"><span>Why we need layers</span></a></li><li><a class="tocitem" href="#What-are-activation-functions"><span>What are activation functions</span></a></li><li><a class="tocitem" href="#Training-neural-networks"><span>Training neural networks</span></a></li><li><a class="tocitem" href="#Predicting-results"><span>Predicting results</span></a></li><li class="toplevel"><a class="tocitem" href="#p-value,-null-hypothesis-and-real-time-analytics-(onlinestat)"><span>p-value, null hypothesis and real time analytics (onlinestat)</span></a></li><li><a class="tocitem" href="#Finance-Ledger-,-Balance-Sheet,-Income-Statement-and-Cash-Flow"><span>Finance Ledger , Balance Sheet, Income Statement and Cash Flow</span></a></li></ul></li><li><a class="tocitem" href="../optimization/">Optimization</a></li><li><a class="tocitem" href="../derivatives/">Derivative &amp; Gradients</a></li><li><a class="tocitem" href="../dl/">Deep Learning</a></li></ul></li><li><a class="tocitem" href="../nlp/">NLP for GL</a></li><li><a class="tocitem" href="../transformers/">Transformers</a></li><li><a class="tocitem" href="../graph/">Graph Theory / Network Science</a></li><li><a class="tocitem" href="../chatgpt/">Open AI ChatGPT</a></li><li><a class="tocitem" href="../elt/">ELT vs ETL</a></li></ul></li><li><a class="tocitem" href="../../api/">API</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Tutorials</a></li><li><a class="is-disabled">ML4GL</a></li><li class="is-active"><a href>p-value, null hypothesis and real time analytic</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>p-value, null hypothesis and real time analytic</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/AmitXShukla/GeneralLedger.jl/blob/master/docs/src/tutorials/hypothesis.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h2 id="Hypothesis,-p-value"><a class="docs-heading-anchor" href="#Hypothesis,-p-value">Hypothesis, p - value</a><a id="Hypothesis,-p-value-1"></a><a class="docs-heading-anchor-permalink" href="#Hypothesis,-p-value" title="Permalink"></a></h2><h2 id="what-is-a-gradient,-derivatives,-gradients,-Jacobians,-Hessians"><a class="docs-heading-anchor" href="#what-is-a-gradient,-derivatives,-gradients,-Jacobians,-Hessians">what is a gradient, derivatives, gradients, Jacobians, Hessians</a><a id="what-is-a-gradient,-derivatives,-gradients,-Jacobians,-Hessians-1"></a><a class="docs-heading-anchor-permalink" href="#what-is-a-gradient,-derivatives,-gradients,-Jacobians,-Hessians" title="Permalink"></a></h2><h2 id="what-is-optimization"><a class="docs-heading-anchor" href="#what-is-optimization">what is optimization</a><a id="what-is-optimization-1"></a><a class="docs-heading-anchor-permalink" href="#what-is-optimization" title="Permalink"></a></h2><h2 id="ForwardDiff,-ReverseDiff"><a class="docs-heading-anchor" href="#ForwardDiff,-ReverseDiff">ForwardDiff, ReverseDiff</a><a id="ForwardDiff,-ReverseDiff-1"></a><a class="docs-heading-anchor-permalink" href="#ForwardDiff,-ReverseDiff" title="Permalink"></a></h2><h2 id="ChainRules,-AutoDiff"><a class="docs-heading-anchor" href="#ChainRules,-AutoDiff">ChainRules, AutoDiff</a><a id="ChainRules,-AutoDiff-1"></a><a class="docs-heading-anchor-permalink" href="#ChainRules,-AutoDiff" title="Permalink"></a></h2><p>AutoGrad | AutoDiff (automatic differentiation)</p><h2 id="Optimization-using-gradient"><a class="docs-heading-anchor" href="#Optimization-using-gradient">Optimization using gradient</a><a id="Optimization-using-gradient-1"></a><a class="docs-heading-anchor-permalink" href="#Optimization-using-gradient" title="Permalink"></a></h2><h2 id="what-is-gradient-descent"><a class="docs-heading-anchor" href="#what-is-gradient-descent">what is gradient descent</a><a id="what-is-gradient-descent-1"></a><a class="docs-heading-anchor-permalink" href="#what-is-gradient-descent" title="Permalink"></a></h2><h2 id="UAT-Universal-Approximation-theorem"><a class="docs-heading-anchor" href="#UAT-Universal-Approximation-theorem">UAT Universal Approximation theorem</a><a id="UAT-Universal-Approximation-theorem-1"></a><a class="docs-heading-anchor-permalink" href="#UAT-Universal-Approximation-theorem" title="Permalink"></a></h2><p>As per Wikipedia -</p><p>In the mathematical theory of artificial neural networks, universal approximation theorems are results, that establish the density of an algorithmically generated class of functions within a given function space of interest.</p><p>In simple English..</p><p>if you set knobs, levers (aka parameters..) of a given UAT function in such a way, this UAT function starts working as the magical function described above (i.e. universal approximation).</p><h2 id="Linear-regression"><a class="docs-heading-anchor" href="#Linear-regression">Linear regression</a><a id="Linear-regression-1"></a><a class="docs-heading-anchor-permalink" href="#Linear-regression" title="Permalink"></a></h2><h2 id="Taylor-Series"><a class="docs-heading-anchor" href="#Taylor-Series">Taylor Series</a><a id="Taylor-Series-1"></a><a class="docs-heading-anchor-permalink" href="#Taylor-Series" title="Permalink"></a></h2><h2 id="Fourier-Transformation"><a class="docs-heading-anchor" href="#Fourier-Transformation">Fourier Transformation</a><a id="Fourier-Transformation-1"></a><a class="docs-heading-anchor-permalink" href="#Fourier-Transformation" title="Permalink"></a></h2><h2 id="Loss-function"><a class="docs-heading-anchor" href="#Loss-function">Loss function</a><a id="Loss-function-1"></a><a class="docs-heading-anchor-permalink" href="#Loss-function" title="Permalink"></a></h2><h2 id="Gradient-and-Gradient-Descent"><a class="docs-heading-anchor" href="#Gradient-and-Gradient-Descent">Gradient &amp; Gradient Descent</a><a id="Gradient-and-Gradient-Descent-1"></a><a class="docs-heading-anchor-permalink" href="#Gradient-and-Gradient-Descent" title="Permalink"></a></h2><h2 id="Curse-of-Dimensionality"><a class="docs-heading-anchor" href="#Curse-of-Dimensionality">Curse of Dimensionality</a><a id="Curse-of-Dimensionality-1"></a><a class="docs-heading-anchor-permalink" href="#Curse-of-Dimensionality" title="Permalink"></a></h2><h2 id="what-is-a-Neural-network"><a class="docs-heading-anchor" href="#what-is-a-Neural-network">what is a Neural network</a><a id="what-is-a-Neural-network-1"></a><a class="docs-heading-anchor-permalink" href="#what-is-a-Neural-network" title="Permalink"></a></h2><h2 id="Neurons"><a class="docs-heading-anchor" href="#Neurons">Neurons</a><a id="Neurons-1"></a><a class="docs-heading-anchor-permalink" href="#Neurons" title="Permalink"></a></h2><h2 id="Why-we-need-layers"><a class="docs-heading-anchor" href="#Why-we-need-layers">Why we need layers</a><a id="Why-we-need-layers-1"></a><a class="docs-heading-anchor-permalink" href="#Why-we-need-layers" title="Permalink"></a></h2><h2 id="What-are-activation-functions"><a class="docs-heading-anchor" href="#What-are-activation-functions">What are activation functions</a><a id="What-are-activation-functions-1"></a><a class="docs-heading-anchor-permalink" href="#What-are-activation-functions" title="Permalink"></a></h2><h2 id="Training-neural-networks"><a class="docs-heading-anchor" href="#Training-neural-networks">Training neural networks</a><a id="Training-neural-networks-1"></a><a class="docs-heading-anchor-permalink" href="#Training-neural-networks" title="Permalink"></a></h2><h2 id="Predicting-results"><a class="docs-heading-anchor" href="#Predicting-results">Predicting results</a><a id="Predicting-results-1"></a><a class="docs-heading-anchor-permalink" href="#Predicting-results" title="Permalink"></a></h2><p>What is a neural network A neural network is the magical function: NN(x)= W(n)σ(n-1)…..(W3σ2(W2σ1(W1x+b1)+b2)+b3…. bn) where W &amp; b (weight &amp; bias) are changeable, represent parameters of a given layer in multi &quot;n&quot; layer function and when these knobs &amp; lever, aka parameters(weights, bias) are trained to tune properly, can produce a rationally acceptable output given a set of input values.</p><p>For the beginner, let&#39;s simplify this and work on a single layer.</p><p>NN(x) = W1x + b</p><p>How computer works and what is GPU</p><p>Stack vs HEAP</p><p>Programming Languages and ML Frameworks</p><p>UAT This means that NN(x) is now a very good function approximator to f(x) = ones(5)! So Why Machine Learning? Why Neural Networks? All we did was find parameters that made NN(x) act like a function f(x). How does that relate to machine learning? Well, in any case where one is acting on data (x,y), the idea is to assume that there exists some underlying mathematical model f(x) = y. If we had perfect knowledge of what f is, then from only the information of x we can then predict what y would be. The inference problem is to then figure out what function f should be. Therefore, machine learning on data is simply this problem of finding an approximator to some unknown function! So why neural networks? Neural networks satisfy two properties. The first of which is known as the Universal Approximation Theorem (UAT), which in simple non-mathematical language means that, for any ϵ of accuracy, if your neural network is large enough (has enough layers, the weight matrices are large enough), then it can approximate any (nice) function f within that ϵ. Therefore, we can reduce the problem of finding missing functions, the problem of machine learning, to a problem of finding the weights of neural networks, which is a well-defined mathematical optimization problem. Why neural networks specifically? That&#39;s a fairly good question, since there are many other functions with this property. For example, you will have learned from analysis that a0+a1x+a2x2+…a0+a1x+a2x2+… arbitrary polynomials can be used to approximate any analytic function (this is the Taylor series). Similarly, a Fourier series f(x)=a0+∑kbkcos(kx)+cksin(kx)f(x)=a0+∑kbkcos⁡(kx)+cksin⁡(kx) can approximate any continuous function f (and discontinuous functions also can have convergence, etc. these are the details of a harmonic analysis course). That&#39;s all for one dimension. How about two dimensional functions? It turns out it&#39;s not difficult to prove that tensor products of universal approximators will give higher dimensional universal approximators. So for example, tensoring together two polynomials: a0+a1x+a2y+a3xy+a4x2y+a5xy2+a6x2y2+…a0+a1x+a2y+a3xy+a4x2y+a5xy2+a6x2y2+… will give a two-dimensional function approximator. But notice how we have to resolve every combination of terms. This means that if we used n coefficients in each dimension d, the total number of coefficients to build a d-dimensional universal approximator from one-dimensional objects would need ndnd coefficients. This exponential growth is known as the curse of dimensionality.</p><p>A fundamental problem that makes language modeling and other learning problems difficult is the curse of dimensionality. It is particularly obvious in the case when one wants to model the joint distribution between many discrete random variables (such as words in a sentence, or discrete at- tributes in a data-mining task). For example, if one wants to model the joint distribution of 10 consecutive words in a natural language with a vocabulary V of size 100,000, there are potentially 100 00010 − 1 = 1050 − 1 free parameters.</p><p>From <a href="https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf">https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf</a></p><p>NN Function Background Neural networks (NNs) are a collection of nested functions that are executed on some input data. These functions are defined by parameters (consisting of weights and biases), which in PyTorch are stored in tensors. Training a NN happens in two steps:</p><p>Gradient</p><p>Gradient descent</p><p>Forward pass copied from PyTorch Forward Propagation: In forward prop, the NN makes its best guess about the correct output. It runs the input data through each of its functions to make this guess.</p><p>Backward propagation copied from PyTorch Backward Propagation: In backprop, the NN adjusts its parameters proportionate to the error in its guess. It does this by traversing backwards from the output, collecting the derivatives of the error with respect to the parameters of the functions (gradients), and optimizing the parameters using gradient descent. For a more detailed walk through of backprop, check out this video from 3Blue1Brown.</p><p>Auto Grad</p><p>Automatic Differentiation</p><p>Loss function</p><p>Generalization</p><p>Regularization</p><p>Optimization</p><p>Epoch</p><p>Layers Why we need so many layers?</p><p>Activation functions Why we need activation functions in NN</p><p>Training Testing Predictions</p><hr/><h1 id="p-value,-null-hypothesis-and-real-time-analytics-(onlinestat)"><a class="docs-heading-anchor" href="#p-value,-null-hypothesis-and-real-time-analytics-(onlinestat)">p-value, null hypothesis and real time analytics (onlinestat)</a><a id="p-value,-null-hypothesis-and-real-time-analytics-(onlinestat)-1"></a><a class="docs-heading-anchor-permalink" href="#p-value,-null-hypothesis-and-real-time-analytics-(onlinestat)" title="Permalink"></a></h1><p>In this section, we will see an example how to perform null hypthoses/p-value analysis on Ledger data.</p><p><em>use case</em></p><p>Finance Ledger</p><h2 id="Finance-Ledger-,-Balance-Sheet,-Income-Statement-and-Cash-Flow"><a class="docs-heading-anchor" href="#Finance-Ledger-,-Balance-Sheet,-Income-Statement-and-Cash-Flow">Finance Ledger , Balance Sheet, Income Statement and Cash Flow</a><a id="Finance-Ledger-,-Balance-Sheet,-Income-Statement-and-Cash-Flow-1"></a><a class="docs-heading-anchor-permalink" href="#Finance-Ledger-,-Balance-Sheet,-Income-Statement-and-Cash-Flow" title="Permalink"></a></h2><p>below is sample Finance Ledger Data, in this Ledger data, we will run p-value to test following Hypothesis.</p><p><code>For a Given Given FISCALY_YEAR and ACCOUNTING_PERIOD, OPERATING EXPENSES are aligned (10%) tolerance range in comparison to BEFORE or AFTER FISCAL_YEAR &amp; ACCOUNTING_PERIOD.</code></p><pre><code class="language-julia-repl hljs" style="display:block;">julia&gt; using DataFrames, Plots, Dates</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; # create dummy data
       accounts = DataFrame(AS_OF_DATE=Date(&quot;1900-01-01&quot;, dateformat&quot;y-m-d&quot;),
            ID = 11000:1000:45000,
            CLASSIFICATION=repeat([
         &quot;OPERATING_EXPENSES&quot;,&quot;NON-OPERATING_EXPENSES&quot;, &quot;ASSETS&quot;,&quot;LIABILITIES&quot;,
         &quot;NET_WORTH&quot;,&quot;STATISTICS&quot;,&quot;REVENUE&quot;
         ], inner=5),
        CATEGORY=[
         &quot;Travel&quot;,&quot;Payroll&quot;,&quot;non-Payroll&quot;,&quot;Allowance&quot;,&quot;Cash&quot;,
         &quot;Facility&quot;,&quot;Supply&quot;,&quot;Services&quot;,&quot;Investment&quot;,&quot;Misc.&quot;,
         &quot;Depreciation&quot;,&quot;Gain&quot;,&quot;Service&quot;,&quot;Retired&quot;,&quot;Fault.&quot;,
         &quot;Receipt&quot;,&quot;Accrual&quot;,&quot;Return&quot;,&quot;Credit&quot;,&quot;ROI&quot;,
         &quot;Cash&quot;,&quot;Funds&quot;,&quot;Invest&quot;,&quot;Transfer&quot;,&quot;Roll-over&quot;,
         &quot;FTE&quot;,&quot;Members&quot;,&quot;Non_Members&quot;,&quot;Temp&quot;,&quot;Contractors&quot;,
         &quot;Sales&quot;,&quot;Merchant&quot;,&quot;Service&quot;,&quot;Consulting&quot;,&quot;Subscriptions&quot;
        ],
        STATUS=&quot;A&quot;,
        DESCR=repeat([
         &quot;operating expenses&quot;,&quot;non-operating expenses&quot;,
         &quot;assets&quot;,&quot;liability&quot;,&quot;net-worth&quot;,&quot;stats&quot;,&quot;revenue&quot;
        ], inner=5),
        ACCOUNT_TYPE=repeat([
        &quot;E&quot;,&quot;E&quot;,&quot;A&quot;,&quot;L&quot;,&quot;N&quot;,&quot;S&quot;,&quot;R&quot;
           ],inner=5));</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; dept = DataFrame(AS_OF_DATE=Date(&quot;2000-01-01&quot;, dateformat&quot;y-m-d&quot;),
              ID = 1100:100:1500,
              CLASSIFICATION=[
        &quot;SALES&quot;,&quot;HR&quot;, &quot;IT&quot;,&quot;BUSINESS&quot;,&quot;OTHERS&quot;
        ],
              CATEGORY=[
        &quot;sales&quot;,&quot;human_resource&quot;,&quot;IT_Staff&quot;,&quot;business&quot;,&quot;others&quot;
        ],
              STATUS=&quot;A&quot;,
              DESCR=[
        &quot;Sales &amp; Marketing&quot;,&quot;Human Resource&quot;,&quot;Infomration Technology&quot;,&quot;Business leaders&quot;,&quot;other temp&quot;
        ],
              DEPT_TYPE=[
        &quot;S&quot;,&quot;H&quot;,&quot;I&quot;,&quot;B&quot;,&quot;O&quot;]);</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; location = DataFrame(AS_OF_DATE=Date(&quot;2000-01-01&quot;, dateformat&quot;y-m-d&quot;),
              ID = 11:1:22,
              CLASSIFICATION=repeat([
        &quot;Region A&quot;,&quot;Region B&quot;, &quot;Region C&quot;], inner=4),
              CATEGORY=repeat([
        &quot;Region A&quot;,&quot;Region B&quot;, &quot;Region C&quot;], inner=4),
              STATUS=&quot;A&quot;,
              DESCR=[
       &quot;Boston&quot;,&quot;New York&quot;,&quot;Philadelphia&quot;,&quot;Cleveland&quot;,&quot;Richmond&quot;,
       &quot;Atlanta&quot;,&quot;Chicago&quot;,&quot;St. Louis&quot;,&quot;Minneapolis&quot;,&quot;Kansas City&quot;,
       &quot;Dallas&quot;,&quot;San Francisco&quot;],
              LOCA_TYPE=&quot;Physical&quot;);</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; ledger = DataFrame(
         LEDGER = String[], FISCAL_YEAR = Int[], PERIOD = Int[], ORGID = String[],
         OPER_UNIT = String[], ACCOUNT = Int[], DEPT = Int[], LOCATION = Int[],
         POSTED_TOTAL = Float64[]
        );</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt;  # create 2020 Period 1-12 Actuals Ledger
        l = &quot;Actuals&quot;;</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt;  fy = 2020;</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt;  for p = 1:12
         for i = 1:10^5
         push!(ledger, (l, fy, p, &quot;ABC Inc.&quot;, rand(location.CATEGORY),
          rand(accounts.ID), rand(dept.ID), rand(location.ID), rand()*10^8))
         end
        end</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt;  # create 2021 Period 1-4 Actuals Ledger
        l = &quot;Actuals&quot;;</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt;  fy = 2021;</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt;  for p = 1:4
         for i = 1:10^5
         push!(ledger, (l, fy, p, &quot;ABC Inc.&quot;, rand(location.CATEGORY),
          rand(accounts.ID), rand(dept.ID), rand(location.ID), rand()*10^8))
         end
        end</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt;  # create 2021 Period 1-4 Budget Ledger
        l = &quot;Budget&quot;;</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt;  fy = 2021;</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt;  for p = 1:12
         for i = 1:10^5
         push!(ledger, (l, fy, p, &quot;ABC Inc.&quot;, rand(location.CATEGORY),
          rand(accounts.ID), rand(dept.ID), rand(location.ID), rand()*10^8))
         end
        end</code><code class="nohighlight hljs ansi" style="display:block;"></code><br/><code class="language-julia-repl hljs" style="display:block;">julia&gt; ledger[:,:]</code><code class="nohighlight hljs ansi" style="display:block;">2800000×9 DataFrame
     Row │ LEDGER   FISCAL_YEAR  PERIOD  ORGID     OPER_UNIT  ACCOUNT  DEPT    ⋯
         │ String   Int64        Int64   String    String     Int64    Int64   ⋯
─────────┼──────────────────────────────────────────────────────────────────────
       1 │ Actuals         2020       1  ABC Inc.  Region B     24000   1200   ⋯
       2 │ Actuals         2020       1  ABC Inc.  Region B     16000   1200
       3 │ Actuals         2020       1  ABC Inc.  Region A     22000   1100
       4 │ Actuals         2020       1  ABC Inc.  Region C     22000   1200
       5 │ Actuals         2020       1  ABC Inc.  Region B     26000   1300   ⋯
       6 │ Actuals         2020       1  ABC Inc.  Region B     25000   1100
       7 │ Actuals         2020       1  ABC Inc.  Region A     36000   1300
       8 │ Actuals         2020       1  ABC Inc.  Region C     37000   1200
    ⋮    │    ⋮          ⋮         ⋮        ⋮          ⋮         ⋮       ⋮     ⋱
 2799994 │ Budget          2021      12  ABC Inc.  Region A     11000   1100   ⋯
 2799995 │ Budget          2021      12  ABC Inc.  Region A     34000   1400
 2799996 │ Budget          2021      12  ABC Inc.  Region C     41000   1300
 2799997 │ Budget          2021      12  ABC Inc.  Region B     15000   1200
 2799998 │ Budget          2021      12  ABC Inc.  Region B     20000   1300   ⋯
 2799999 │ Budget          2021      12  ABC Inc.  Region C     31000   1500
 2800000 │ Budget          2021      12  ABC Inc.  Region C     43000   1200
                                              2 columns and 2799985 rows omitted</code></pre><h4 id="p-value-function"><a class="docs-heading-anchor" href="#p-value-function">p-value function</a><a id="p-value-function-1"></a><a class="docs-heading-anchor-permalink" href="#p-value-function" title="Permalink"></a></h4><p>getPValue(ledger, ACCOUNT<em>CLASSIFICATION = &quot;OPERATING</em>EXPENSES&quot;, FISCAL<em>YEAR =2021, ACCOUNTING</em>PERIOD = 7)</p><p>return p-value for given ACCOUNT NODE</p><p>getPValue(ledger, ACCOUNT<em>CLASSIFICATION = &quot;OPERATING</em>REVENUE&quot;, FISCAL<em>YEAR =2021, ACCOUNTING</em>PERIOD = 7)</p><p>return p-value for given ACCOUNT NODE</p><p>getPValue(ledger, ACCOUNT<em>CLASSIFICATION = &quot;OPERATING</em>ASSETS&quot;, FISCAL<em>YEAR =2021, ACCOUNTING</em>PERIOD = 7)</p><p>return p-value for given ACCOUNT NODE</p><p>getPValue(ledger, DEPT<em>CLASSIFICATION = &quot;OPERATING</em>EXPENSES&quot;, FISCAL<em>YEAR =2021, ACCOUNTING</em>PERIOD = 7)</p><p>return p-value for given DEPT NODE</p><p>getPValue(ledger, DEPT<em>CLASSIFICATION = &quot;OPERATING</em>REVENUE&quot;, FISCAL<em>YEAR =2021, ACCOUNTING</em>PERIOD = 7)</p><p>return p-value for given DEPT NODE</p><p>getPValue(ledger, DEPT<em>CLASSIFICATION = &quot;OPERATING</em>ASSETS&quot;, FISCAL<em>YEAR =2021, ACCOUNTING</em>PERIOD = 7)</p><p>return p-value for given DEPT NODE</p><p>getPValue(ledger, REGION<em>CLASSIFICATION = &quot;OPERATING</em>EXPENSES&quot;, FISCAL<em>YEAR =2021, ACCOUNTING</em>PERIOD = 7)</p><p>return p-value for given REGION NODE</p><p>getPValue(ledger, REGION<em>CLASSIFICATION = &quot;OPERATING</em>REVENUE&quot;, FISCAL<em>YEAR =2021, ACCOUNTING</em>PERIOD = 7)</p><p>return p-value for given REGION NODE</p><p>getPValue(ledger, REGION<em>CLASSIFICATION = &quot;OPERATING</em>ASSETS&quot;, FISCAL<em>YEAR =2021, ACCOUNTING</em>PERIOD = 7)</p><p>return p-value for given REGION NODE</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../statistics/">« Statistics</a><a class="docs-footer-nextpage" href="../optimization/">Optimization »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.23 on <span class="colophon-date" title="Friday 10 February 2023 18:25">Friday 10 February 2023</span>. Using Julia version 1.9.0-beta2.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
